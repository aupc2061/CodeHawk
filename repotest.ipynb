{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import itertools\n",
    "import platform\n",
    "from pprint import pprint\n",
    "import shlex\n",
    "import subprocess\n",
    "import tempfile\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sqlite3\n",
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_python as tspython\n",
    "# PY_LANGUAGE = Language(tspython.language())\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from importlib import resources\n",
    "from pathlib import Path\n",
    "\n",
    "from diskcache import Cache\n",
    "from grep_ast import TreeContext, filename_to_lang\n",
    "from pygments.lexers import guess_lexer_for_filename\n",
    "from pygments.token import Token\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tree_sitter_languages import get_language, get_parser\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "Tag = namedtuple(\"Tag\", \"rel_fname fname line name kind\".split())\n",
    "\n",
    "ROOT_IMPORTANT_FILES = [\n",
    "    # Version Control\n",
    "    \".gitignore\",\n",
    "    \".gitattributes\",\n",
    "    # Documentation\n",
    "    \"README\",\n",
    "    \"README.md\",\n",
    "    \"README.txt\",\n",
    "    \"README.rst\",\n",
    "    \"CONTRIBUTING\",\n",
    "    \"CONTRIBUTING.md\",\n",
    "    \"CONTRIBUTING.txt\",\n",
    "    \"CONTRIBUTING.rst\",\n",
    "    \"LICENSE\",\n",
    "    \"LICENSE.md\",\n",
    "    \"LICENSE.txt\",\n",
    "    \"CHANGELOG\",\n",
    "    \"CHANGELOG.md\",\n",
    "    \"CHANGELOG.txt\",\n",
    "    \"CHANGELOG.rst\",\n",
    "    \"SECURITY\",\n",
    "    \"SECURITY.md\",\n",
    "    \"SECURITY.txt\",\n",
    "    \"CODEOWNERS\",\n",
    "    # Package Management and Dependencies\n",
    "    \"requirements.txt\",\n",
    "    \"Pipfile\",\n",
    "    \"Pipfile.lock\",\n",
    "    \"pyproject.toml\",\n",
    "    \"setup.py\",\n",
    "    \"setup.cfg\",\n",
    "    \"package.json\",\n",
    "    \"package-lock.json\",\n",
    "    \"yarn.lock\",\n",
    "    \"npm-shrinkwrap.json\",\n",
    "    \"Gemfile\",\n",
    "    \"Gemfile.lock\",\n",
    "    \"composer.json\",\n",
    "    \"composer.lock\",\n",
    "    \"pom.xml\",\n",
    "    \"build.gradle\",\n",
    "    \"build.sbt\",\n",
    "    \"go.mod\",\n",
    "    \"go.sum\",\n",
    "    \"Cargo.toml\",\n",
    "    \"Cargo.lock\",\n",
    "    \"mix.exs\",\n",
    "    \"rebar.config\",\n",
    "    \"project.clj\",\n",
    "    \"Podfile\",\n",
    "    \"Cartfile\",\n",
    "    \"dub.json\",\n",
    "    \"dub.sdl\",\n",
    "    # Configuration and Settings\n",
    "    \".env\",\n",
    "    \".env.example\",\n",
    "    \".editorconfig\",\n",
    "    \"tsconfig.json\",\n",
    "    \"jsconfig.json\",\n",
    "    \".babelrc\",\n",
    "    \"babel.config.js\",\n",
    "    \".eslintrc\",\n",
    "    \".eslintignore\",\n",
    "    \".prettierrc\",\n",
    "    \".stylelintrc\",\n",
    "    \"tslint.json\",\n",
    "    \".pylintrc\",\n",
    "    \".flake8\",\n",
    "    \".rubocop.yml\",\n",
    "    \".scalafmt.conf\",\n",
    "    \".dockerignore\",\n",
    "    \".gitpod.yml\",\n",
    "    \"sonar-project.properties\",\n",
    "    \"renovate.json\",\n",
    "    \"dependabot.yml\",\n",
    "    \".pre-commit-config.yaml\",\n",
    "    \"mypy.ini\",\n",
    "    \"tox.ini\",\n",
    "    \".yamllint\",\n",
    "    \"pyrightconfig.json\",\n",
    "    # Build and Compilation\n",
    "    \"webpack.config.js\",\n",
    "    \"rollup.config.js\",\n",
    "    \"parcel.config.js\",\n",
    "    \"gulpfile.js\",\n",
    "    \"Gruntfile.js\",\n",
    "    \"build.xml\",\n",
    "    \"build.boot\",\n",
    "    \"project.json\",\n",
    "    \"build.cake\",\n",
    "    \"MANIFEST.in\",\n",
    "    # Testing\n",
    "    \"pytest.ini\",\n",
    "    \"phpunit.xml\",\n",
    "    \"karma.conf.js\",\n",
    "    \"jest.config.js\",\n",
    "    \"cypress.json\",\n",
    "    \".nycrc\",\n",
    "    \".nycrc.json\",\n",
    "    # CI/CD\n",
    "    \".travis.yml\",\n",
    "    \".gitlab-ci.yml\",\n",
    "    \"Jenkinsfile\",\n",
    "    \"azure-pipelines.yml\",\n",
    "    \"bitbucket-pipelines.yml\",\n",
    "    \"appveyor.yml\",\n",
    "    \"circle.yml\",\n",
    "    \".circleci/config.yml\",\n",
    "    \".github/dependabot.yml\",\n",
    "    \"codecov.yml\",\n",
    "    \".coveragerc\",\n",
    "    # Docker and Containers\n",
    "    \"Dockerfile\",\n",
    "    \"docker-compose.yml\",\n",
    "    \"docker-compose.override.yml\",\n",
    "    # Cloud and Serverless\n",
    "    \"serverless.yml\",\n",
    "    \"firebase.json\",\n",
    "    \"now.json\",\n",
    "    \"netlify.toml\",\n",
    "    \"vercel.json\",\n",
    "    \"app.yaml\",\n",
    "    \"terraform.tf\",\n",
    "    \"main.tf\",\n",
    "    \"cloudformation.yaml\",\n",
    "    \"cloudformation.json\",\n",
    "    \"ansible.cfg\",\n",
    "    \"kubernetes.yaml\",\n",
    "    \"k8s.yaml\",\n",
    "    # Database\n",
    "    \"schema.sql\",\n",
    "    \"liquibase.properties\",\n",
    "    \"flyway.conf\",\n",
    "    # Framework-specific\n",
    "    \"next.config.js\",\n",
    "    \"nuxt.config.js\",\n",
    "    \"vue.config.js\",\n",
    "    \"angular.json\",\n",
    "    \"gatsby-config.js\",\n",
    "    \"gridsome.config.js\",\n",
    "    # API Documentation\n",
    "    \"swagger.yaml\",\n",
    "    \"swagger.json\",\n",
    "    \"openapi.yaml\",\n",
    "    \"openapi.json\",\n",
    "    # Development environment\n",
    "    \".nvmrc\",\n",
    "    \".ruby-version\",\n",
    "    \".python-version\",\n",
    "    \"Vagrantfile\",\n",
    "    # Quality and metrics\n",
    "    \".codeclimate.yml\",\n",
    "    \"codecov.yml\",\n",
    "    # Documentation\n",
    "    \"mkdocs.yml\",\n",
    "    \"_config.yml\",\n",
    "    \"book.toml\",\n",
    "    \"readthedocs.yml\",\n",
    "    \".readthedocs.yaml\",\n",
    "    # Package registries\n",
    "    \".npmrc\",\n",
    "    \".yarnrc\",\n",
    "    # Linting and formatting\n",
    "    \".isort.cfg\",\n",
    "    \".markdownlint.json\",\n",
    "    \".markdownlint.yaml\",\n",
    "    # Security\n",
    "    \".bandit\",\n",
    "    \".secrets.baseline\",\n",
    "    # Misc\n",
    "    \".pypirc\",\n",
    "    \".gitkeep\",\n",
    "    \".npmignore\",\n",
    "]\n",
    "\n",
    "\n",
    "# Normalize the lists once\n",
    "NORMALIZED_ROOT_IMPORTANT_FILES = set(os.path.normpath(path) for path in ROOT_IMPORTANT_FILES)\n",
    "\n",
    "class Spinner:\n",
    "    unicode_spinner = [\"⠋\", \"⠙\", \"⠹\", \"⠸\", \"⠼\", \"⠴\", \"⠦\", \"⠧\", \"⠇\", \"⠏\"]\n",
    "    ascii_spinner = [\"|\", \"/\", \"-\", \"\\\\\"]\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.start_time = time.time()\n",
    "        self.last_update = 0\n",
    "        self.visible = False\n",
    "        self.is_tty = sys.stdout.isatty()\n",
    "        self.tested = False\n",
    "\n",
    "    def test_charset(self):\n",
    "        if self.tested:\n",
    "            return\n",
    "        self.tested = True\n",
    "        # Try unicode first, fall back to ascii if needed\n",
    "        try:\n",
    "            # Test if we can print unicode characters\n",
    "            print(self.unicode_spinner[0], end=\"\", flush=True)\n",
    "            print(\"\\r\", end=\"\", flush=True)\n",
    "            self.spinner_chars = itertools.cycle(self.unicode_spinner)\n",
    "        except UnicodeEncodeError:\n",
    "            self.spinner_chars = itertools.cycle(self.ascii_spinner)\n",
    "\n",
    "    def step(self):\n",
    "        if not self.is_tty:\n",
    "            return\n",
    "\n",
    "        current_time = time.time()\n",
    "        if not self.visible and current_time - self.start_time >= 0.5:\n",
    "            self.visible = True\n",
    "            self._step()\n",
    "        elif self.visible and current_time - self.last_update >= 0.1:\n",
    "            self._step()\n",
    "        self.last_update = current_time\n",
    "\n",
    "    def _step(self):\n",
    "        if not self.visible:\n",
    "            return\n",
    "\n",
    "        self.test_charset()\n",
    "        print(f\"\\r{self.text} {next(self.spinner_chars)}\\r{self.text} \", end=\"\", flush=True)\n",
    "\n",
    "    def end(self):\n",
    "        if self.visible and self.is_tty:\n",
    "            print(\"\\r\" + \" \" * (len(self.text) + 3))\n",
    "\n",
    "def is_important(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    dir_name = os.path.normpath(os.path.dirname(file_path))\n",
    "    normalized_path = os.path.normpath(file_path)\n",
    "\n",
    "    # Check for GitHub Actions workflow files\n",
    "    if dir_name == os.path.normpath(\".github/workflows\") and file_name.endswith(\".yml\"):\n",
    "        return True\n",
    "\n",
    "    return normalized_path in NORMALIZED_ROOT_IMPORTANT_FILES\n",
    "\n",
    "\n",
    "def filter_important_files(file_paths):\n",
    "    \"\"\"\n",
    "    Filter a list of file paths to return only those that are commonly important in codebases.\n",
    "\n",
    "    :param file_paths: List of file paths to check\n",
    "    :return: List of file paths that match important file patterns\n",
    "    \"\"\"\n",
    "    return list(filter(is_important, file_paths))\n",
    "\n",
    "def get_test(fname):\n",
    "    lang = filename_to_lang(fname)\n",
    "    print(lang)\n",
    "    language = get_language(lang)\n",
    "    print(language)\n",
    "    parser = get_parser(lang)\n",
    "    print(parser)\n",
    "    \n",
    "    \n",
    "def get_scm_fname(lang):\n",
    "    # Load the tags queries\n",
    "    try:\n",
    "        # base_path = Path(__file__).parent\n",
    "        base_path = Path(os.getcwd())\n",
    "        return base_path.joinpath(\"queries\", f\"tree-sitter-{lang}-tags.scm\")\n",
    "    except KeyError:\n",
    "        return\n",
    "\n",
    "\n",
    "def get_tags_raw(fname, rel_fname):\n",
    "        lang = filename_to_lang(fname)\n",
    "        if not lang:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            language = get_language(lang)\n",
    "            parser = get_parser(lang)\n",
    "        except Exception as err:\n",
    "            print(f\"Skipping file {fname}: {err}\")\n",
    "            return\n",
    "\n",
    "        query_scm = get_scm_fname(lang)\n",
    "        if not query_scm.exists():\n",
    "            return\n",
    "        query_scm = query_scm.read_text()\n",
    "\n",
    "        # Read source code\n",
    "        with open(fname, 'r', encoding=\"utf-8\") as f:\n",
    "            code = f.read()\n",
    "        if not code:\n",
    "            return\n",
    "        tree = parser.parse(bytes(code, \"utf-8\"))\n",
    "\n",
    "        # Run the tags queries\n",
    "        query = language.query(query_scm)\n",
    "        captures = query.captures(tree.root_node)\n",
    "\n",
    "        captures = list(captures)\n",
    "\n",
    "        saw = set()\n",
    "        for node, tag in captures:\n",
    "            if tag.startswith(\"name.definition.\"):\n",
    "                kind = \"def\"\n",
    "            elif tag.startswith(\"name.reference.\"):\n",
    "                kind = \"ref\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            saw.add(kind)\n",
    "\n",
    "            result = Tag(\n",
    "                rel_fname=rel_fname,\n",
    "                fname=fname,\n",
    "                name=node.text.decode(\"utf-8\"),\n",
    "                kind=kind,\n",
    "                line=node.start_point[0],\n",
    "            )\n",
    "\n",
    "            yield result\n",
    "\n",
    "        if \"ref\" in saw:\n",
    "            return\n",
    "        if \"def\" not in saw:\n",
    "            return\n",
    "\n",
    "        # We saw defs, without any refs\n",
    "        # Some tags files only provide defs (cpp, for example)\n",
    "        # Use pygments to backfill refs\n",
    "\n",
    "        try:\n",
    "            lexer = guess_lexer_for_filename(fname, code)\n",
    "        except Exception:  # On Windows, bad ref to time.clock which is deprecated?\n",
    "            # self.io.tool_error(f\"Error lexing {fname}\")\n",
    "            print(\"Error lexing {fname}\")\n",
    "            return\n",
    "\n",
    "        tokens = list(lexer.get_tokens(code))\n",
    "        tokens = [token[1] for token in tokens if token[0] in Token.Name]\n",
    "\n",
    "        for token in tokens:\n",
    "            yield Tag(\n",
    "                rel_fname=rel_fname,\n",
    "                fname=fname,\n",
    "                name=token,\n",
    "                kind=\"ref\",\n",
    "                line=-1,\n",
    "            )\n",
    "            \n",
    "def get_rel_fname(fname, root=\"D:\\Projects\\llmpairprog\\Agentless\\playground\\588436a5-43f1-45f4-80b4-0394f1d7b838\\matplotlib\"):\n",
    "        try:\n",
    "            return os.path.relpath(fname, root)\n",
    "        except ValueError:\n",
    "            # Issue #1288: ValueError: path is on mount 'C:', start on mount 'D:'\n",
    "            # Just return the full fname.\n",
    "            return fname\n",
    "        \n",
    "def get_tags(fname, rel_fname):\n",
    "        \"\"\"Get tags for a single file\"\"\"\n",
    "        data = list(get_tags_raw(fname, rel_fname))\n",
    "\n",
    "        return data\n",
    "            \n",
    "def get_ranked_tags(\n",
    "    chat_fnames, other_fnames, mentioned_fnames, mentioned_idents, progress=None\n",
    "):\n",
    "    import networkx as nx\n",
    "\n",
    "    defines = defaultdict(set)\n",
    "    references = defaultdict(list)\n",
    "    definitions = defaultdict(set)\n",
    "\n",
    "    personalization = dict()\n",
    "\n",
    "    fnames = set(chat_fnames).union(set(other_fnames))\n",
    "    chat_rel_fnames = set()\n",
    "\n",
    "    fnames = sorted(fnames)\n",
    "\n",
    "    # Default personalization for unspecified files is 1/num_nodes\n",
    "    # https://networkx.org/documentation/stable/_modules/networkx/algorithms/link_analysis/pagerank_alg.html#pagerank\n",
    "    personalize = 100 / len(fnames)\n",
    "\n",
    "    showing_bar = False\n",
    "\n",
    "    for fname in fnames:\n",
    "        # if self.verbose:\n",
    "        #     self.io.tool_output(f\"Processing {fname}\")\n",
    "        if progress and not showing_bar:\n",
    "            progress()\n",
    "\n",
    "        try:\n",
    "            file_ok = Path(fname).is_file()\n",
    "        except OSError:\n",
    "            file_ok = False\n",
    "\n",
    "        # if not file_ok:\n",
    "        #     if fname not in self.warned_files:\n",
    "        #         self.io.tool_warning(f\"Repo-map can't include {fname}\")\n",
    "        #         self.io.tool_output(\n",
    "        #             \"Has it been deleted from the file system but not from git?\"\n",
    "        #         )\n",
    "        #         self.warned_files.add(fname)\n",
    "        #     continue\n",
    "\n",
    "        # dump(fname)\n",
    "        rel_fname = get_rel_fname(fname)\n",
    "\n",
    "        if fname in chat_fnames:\n",
    "            personalization[rel_fname] = personalize\n",
    "            chat_rel_fnames.add(rel_fname)\n",
    "\n",
    "        if rel_fname in mentioned_fnames:\n",
    "            personalization[rel_fname] = personalize\n",
    "\n",
    "        tags = list(get_tags(fname, rel_fname))\n",
    "        if tags is None:\n",
    "            continue\n",
    "\n",
    "        for tag in tags:\n",
    "            if tag.kind == \"def\":\n",
    "                defines[tag.name].add(rel_fname)\n",
    "                key = (rel_fname, tag.name)\n",
    "                definitions[key].add(tag)\n",
    "\n",
    "            elif tag.kind == \"ref\":\n",
    "                references[tag.name].append(rel_fname)\n",
    "\n",
    "    ##\n",
    "    # dump(defines)\n",
    "    # dump(references)\n",
    "    # dump(personalization)\n",
    "\n",
    "    if not references:\n",
    "        references = dict((k, list(v)) for k, v in defines.items())\n",
    "\n",
    "    idents = set(defines.keys()).intersection(set(references.keys()))\n",
    "\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    for ident in idents:\n",
    "        if progress:\n",
    "            progress()\n",
    "\n",
    "        definers = defines[ident]\n",
    "        if ident in mentioned_idents:\n",
    "            mul = 10\n",
    "        elif ident.startswith(\"_\"):\n",
    "            mul = 0.1\n",
    "        else:\n",
    "            mul = 1\n",
    "\n",
    "        for referencer, num_refs in Counter(references[ident]).items():\n",
    "            for definer in definers:\n",
    "                # dump(referencer, definer, num_refs, mul)\n",
    "                # if referencer == definer:\n",
    "                #    continue\n",
    "\n",
    "                # scale down so high freq (low value) mentions don't dominate\n",
    "                num_refs = math.sqrt(num_refs)\n",
    "\n",
    "                G.add_edge(referencer, definer, weight=mul * num_refs, ident=ident)\n",
    "\n",
    "    if not references:\n",
    "        pass\n",
    "\n",
    "    if personalization:\n",
    "        pers_args = dict(personalization=personalization, dangling=personalization)\n",
    "    else:\n",
    "        pers_args = dict()\n",
    "\n",
    "    try:\n",
    "        ranked = nx.pagerank(G, weight=\"weight\", **pers_args)\n",
    "    except ZeroDivisionError:\n",
    "        # Issue #1536\n",
    "        try:\n",
    "            ranked = nx.pagerank(G, weight=\"weight\")\n",
    "        except ZeroDivisionError:\n",
    "            return []\n",
    "\n",
    "    # distribute the rank from each source node, across all of its out edges\n",
    "    ranked_definitions = defaultdict(float)\n",
    "    for src in G.nodes:\n",
    "        if progress:\n",
    "            progress()\n",
    "\n",
    "        src_rank = ranked[src]\n",
    "        total_weight = sum(data[\"weight\"] for _src, _dst, data in G.out_edges(src, data=True))\n",
    "        # dump(src, src_rank, total_weight)\n",
    "        for _src, dst, data in G.out_edges(src, data=True):\n",
    "            data[\"rank\"] = src_rank * data[\"weight\"] / total_weight\n",
    "            ident = data[\"ident\"]\n",
    "            ranked_definitions[(dst, ident)] += data[\"rank\"]\n",
    "\n",
    "    ranked_tags = []\n",
    "    ranked_definitions = sorted(\n",
    "        ranked_definitions.items(), reverse=True, key=lambda x: (x[1], x[0])\n",
    "    )\n",
    "\n",
    "    # dump(ranked_definitions)\n",
    "\n",
    "    for (fname, ident), rank in ranked_definitions:\n",
    "        # print(f\"{rank:.03f} {fname} {ident}\")\n",
    "        if fname in chat_rel_fnames:\n",
    "            continue\n",
    "        ranked_tags += list(definitions.get((fname, ident), []))\n",
    "\n",
    "    rel_other_fnames_without_tags = set(get_rel_fname(fname) for fname in other_fnames)\n",
    "\n",
    "    fnames_already_included = set(rt[0] for rt in ranked_tags)\n",
    "\n",
    "    top_rank = sorted([(rank, node) for (node, rank) in ranked.items()], reverse=True)\n",
    "    for rank, fname in top_rank:\n",
    "        if fname in rel_other_fnames_without_tags:\n",
    "            rel_other_fnames_without_tags.remove(fname)\n",
    "        if fname not in fnames_already_included:\n",
    "            ranked_tags.append((fname,))\n",
    "\n",
    "    for fname in rel_other_fnames_without_tags:\n",
    "        ranked_tags.append((fname,))\n",
    "\n",
    "    return ranked_tags\n",
    "\n",
    "def token_count(text):\n",
    "    return len(text.split(\" \"))\n",
    "\n",
    "def get_ranked_tags_map_uncached(\n",
    "        chat_fnames,\n",
    "        other_fnames=None,\n",
    "        max_map_tokens=None,\n",
    "        mentioned_fnames=None,\n",
    "        mentioned_idents=None,\n",
    "    ):\n",
    "        if not other_fnames:\n",
    "            other_fnames = list()\n",
    "        if not max_map_tokens:\n",
    "            max_map_tokens = max_map_tokens\n",
    "        if not mentioned_fnames:\n",
    "            mentioned_fnames = set()\n",
    "        if not mentioned_idents:\n",
    "            mentioned_idents = set()\n",
    "\n",
    "        spin = Spinner(\"Updating repo map\")\n",
    "\n",
    "        ranked_tags = get_ranked_tags(\n",
    "            chat_fnames,\n",
    "            other_fnames,\n",
    "            mentioned_fnames,\n",
    "            mentioned_idents,\n",
    "            progress=spin.step,\n",
    "        )\n",
    "\n",
    "        other_rel_fnames = sorted(set(get_rel_fname(fname) for fname in other_fnames))\n",
    "        special_fnames = filter_important_files(other_rel_fnames)\n",
    "        ranked_tags_fnames = set(tag[0] for tag in ranked_tags)\n",
    "        special_fnames = [fn for fn in special_fnames if fn not in ranked_tags_fnames]\n",
    "        special_fnames = [(fn,) for fn in special_fnames]\n",
    "\n",
    "        ranked_tags = special_fnames + ranked_tags\n",
    "\n",
    "        spin.step()\n",
    "\n",
    "        num_tags = len(ranked_tags)\n",
    "        lower_bound = 0\n",
    "        upper_bound = num_tags\n",
    "        best_tree = None\n",
    "        best_tree_tokens = 0\n",
    "\n",
    "        chat_rel_fnames = set(get_rel_fname(fname) for fname in chat_fnames)\n",
    "        tree = to_tree(ranked_tags[:num_tags], chat_rel_fnames)\n",
    "        num_tokens = token_count(tree)\n",
    "\n",
    "        if num_tokens > max_map_tokens:\n",
    "            print(f\"Warning: The generated tree exceeds the max_map_tokens limit by {num_tokens - max_map_tokens} tokens.\")\n",
    "\n",
    "        return tree\n",
    "        # self.tree_cache = dict()\n",
    "\n",
    "        # middle = min(max_map_tokens // 25, num_tags)\n",
    "        # while lower_bound <= upper_bound:\n",
    "        #     # dump(lower_bound, middle, upper_bound)\n",
    "\n",
    "        #     spin.step()\n",
    "\n",
    "        #     tree = to_tree(ranked_tags[:middle], chat_rel_fnames)\n",
    "        #     num_tokens = token_count(tree)\n",
    "\n",
    "        #     pct_err = abs(num_tokens - max_map_tokens) / max_map_tokens\n",
    "        #     ok_err = 0.15\n",
    "        #     if (num_tokens <= max_map_tokens and num_tokens > best_tree_tokens) or pct_err < ok_err:\n",
    "        #         best_tree = tree\n",
    "        #         best_tree_tokens = num_tokens\n",
    "\n",
    "        #         if pct_err < ok_err:\n",
    "        #             break\n",
    "\n",
    "        #     if num_tokens < max_map_tokens:\n",
    "        #         lower_bound = middle + 1\n",
    "        #     else:\n",
    "        #         upper_bound = middle - 1\n",
    "\n",
    "        #     middle = (lower_bound + upper_bound) // 2\n",
    "\n",
    "        # spin.end()\n",
    "        # return best_tree\n",
    "    \n",
    "def get_mtime(fname):\n",
    "    try:\n",
    "        return os.path.getmtime(fname)\n",
    "    except FileNotFoundError:\n",
    "        # self.io.tool_warning(f\"File not found error: {fname}\")\n",
    "        print(f\"File not found error: {fname}\")\n",
    "\n",
    "def render_tree(abs_fname, rel_fname, lois):\n",
    "    mtime = get_mtime(abs_fname)\n",
    "    key = (rel_fname, tuple(sorted(lois)), mtime)\n",
    "\n",
    "\n",
    "    # code = self.io.read_text(abs_fname) or \"\"\n",
    "    with open(abs_fname, 'r', encoding=\"utf-8\") as f:\n",
    "        code = f.read()\n",
    "    if not code.endswith(\"\\n\"):\n",
    "        code += \"\\n\"\n",
    "\n",
    "    context = TreeContext(\n",
    "        rel_fname,\n",
    "        code,\n",
    "        color=False,\n",
    "        line_number=False,\n",
    "        child_context=False,\n",
    "        last_line=False,\n",
    "        margin=0,\n",
    "        mark_lois=False,\n",
    "        loi_pad=0,\n",
    "        # header_max=30,\n",
    "        show_top_of_file_parent_scope=False,\n",
    "    )\n",
    "        # self.tree_context_cache[rel_fname] = {\"context\": context, \"mtime\": mtime}\n",
    "\n",
    "    # context = self.tree_context_cache[rel_fname][\"context\"]\n",
    "    context.lines_of_interest = set()\n",
    "    context.add_lines_of_interest(lois)\n",
    "    context.add_context()\n",
    "    res = context.format()\n",
    "    # self.tree_cache[key] = res\n",
    "    return res\n",
    "\n",
    "def to_tree(tags, chat_rel_fnames):\n",
    "    # print(\"DEBUG: Starting to_tree\")\n",
    "    # print(f\"DEBUG: Number of tags: {len(tags)}\")\n",
    "    if not tags:\n",
    "        return \"\"\n",
    "\n",
    "    cur_fname = None\n",
    "    cur_abs_fname = None\n",
    "    lois = None\n",
    "    output = \"\"\n",
    "    # add a bogus tag at the end so we trip the this_fname != cur_fname...\n",
    "    # dummy_tag = (None,)\n",
    "    dummy_tag = Tag(rel_fname=\"dummy\", fname=\"dummy\", line=999999, name=\"dummy\", kind=\"dummy\")\n",
    "    # sorted_tags = sorted(tags) + [dummy_tag]\n",
    "    # print(f\"DEBUG: Sorted tags: {sorted_tags[:5]}\")\n",
    "    \n",
    "    \n",
    "    # print(f\"DEBUG: Sorted tags: {sorted_tags[:5]}\")  # Show first 5 tags\n",
    "    for tag in sorted(tags) + [dummy_tag]:\n",
    "        this_rel_fname = tag[0]\n",
    "        # print(f\"DEBUG: Current tag: {tag}\")\n",
    "        # print(f\"DEBUG: this_rel_fname: {this_rel_fname}, cur_fname: {cur_fname}\")\n",
    "        # if this_rel_fname in chat_rel_fnames:\n",
    "        #     continue\n",
    "\n",
    "        # ... here ... to output the final real entry in the list\n",
    "        if this_rel_fname != cur_fname:\n",
    "            # print(f\"DEBUG: File change detected: {this_rel_fname} != {cur_fname}\")\n",
    "            if lois is not None:\n",
    "                output += \"\\n\"\n",
    "                output += cur_fname + \":\\n\"\n",
    "                print(\"abs name\", cur_abs_fname, \"rel name: \", cur_fname)\n",
    "                output += render_tree(cur_abs_fname, cur_fname, lois)\n",
    "                lois = None\n",
    "            elif cur_fname:\n",
    "                output += \"\\n\" + cur_fname + \"\\n\"\n",
    "            if type(tag) is Tag:\n",
    "                lois = []\n",
    "                cur_abs_fname = tag.fname\n",
    "            cur_fname = this_rel_fname\n",
    "\n",
    "        if lois is not None:\n",
    "            lois.append(tag.line)\n",
    "\n",
    "    # truncate long lines, in case we get minified js or something else crazy\n",
    "    output = \"\\n\".join([line[:100] for line in output.splitlines()]) + \"\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "def find_src_files(directory):\n",
    "    if not os.path.isdir(directory):\n",
    "        return [directory]\n",
    "\n",
    "    src_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            src_files.append(os.path.join(root, file))\n",
    "    return src_files\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(class_definition\n",
      "  name: (identifier) @name.definition.class) @definition.class\n",
      "\n",
      "(function_definition\n",
      "  name: (identifier) @name.definition.function) @definition.function\n",
      "\n",
      "(call\n",
      "  function: [\n",
      "      (identifier) @name.reference.call\n",
      "      (attribute\n",
      "        attribute: (identifier) @name.reference.call)\n",
      "  ]) @reference.call\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_path = Path(os.getcwd())\n",
    "lang = \"python\"\n",
    "base_path = base_path.joinpath(\"queries\", f\"tree-sitter-{lang}-tags.scm\")\n",
    "base_path\n",
    "with open(base_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/Projects/aider\"\n",
    "file_paths = [r'D:\\Projects\\Agentless']\n",
    "\n",
    "chat_fnames = []\n",
    "other_fnames = []\n",
    "for fname in file_paths:\n",
    "    if Path(fname).is_dir():\n",
    "        chat_fnames += find_src_files(fname)\n",
    "    else:\n",
    "        chat_fnames.append(fname)\n",
    "# rm = RepoMap(root=\".\", io=io_handler, main_model=mock_model, map_tokens=1024, verbose=False)\n",
    "repo_map = get_ranked_tags_map_uncached(chat_fnames, other_fnames, max_map_tokens=1024)\n",
    "# for fname, tags in repo_map.items():\n",
    "#     print(f\"File: {fname}\")\n",
    "#     print(\"  Definitions:\")\n",
    "#     for tag in tags[\"defs\"]:\n",
    "#         print(f\"    Line {tag.line}: {tag.name}\")\n",
    "#     print(\"  References:\")\n",
    "#     for tag in tags[\"refs\"]:\n",
    "#         print(f\"    Line {tag.line}: {tag.name}\")\n",
    "# tags = list(get_tags_raw(file_path, get_rel_fname(file_path, root_path)))\n",
    "# for tag in tags:\n",
    "#     pprint(tag)\n",
    "# ranked_tags = [\n",
    "#     Tag(rel_fname=\"file1.py\", fname=\"file1.py\", line=10, name=\"func1\", kind=\"def\"),\n",
    "#     Tag(rel_fname=\"file1.py\", fname=\"file1.py\", line=20, name=\"func2\", kind=\"ref\"),\n",
    "#     Tag(rel_fname=\"file2.py\", fname=\"file2.py\", line=30, name=\"func3\", kind=\"def\"),\n",
    "#     Tag(rel_fname=\"file2.py\", fname=\"file2.py\", line=40, name=\"func4\", kind=\"ref\"),\n",
    "# ]\n",
    "# chat_rel_fnames = {\"file1.py\", \"file2.py\"}\n",
    "\n",
    "# tree_output = to_tree(ranked_tags, chat_rel_fnames)\n",
    "repo_map_unique = set(repo_map)\n",
    "# Test code\n",
    "# test_tags = [\n",
    "#     Tag(rel_fname=\"test1.py\", fname=\"test1.py\", line=10, name=\"func1\", kind=\"def\"),\n",
    "#     Tag(rel_fname=\"test1.py\", fname=\"test1.py\", line=20, name=\"func2\", kind=\"ref\"),\n",
    "#     Tag(rel_fname=\"test2.py\", fname=\"test2.py\", line=30, name=\"func3\", kind=\"def\")\n",
    "# ]\n",
    "# chat_rel_fnames = set()\n",
    "# result = to_tree(test_tags, chat_rel_fnames)\n",
    "# print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chat_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "elements = [line.strip().replace(\"..\\\\..\\\\\", \"\") for line in repo_map.splitlines() if line.strip()]\n",
    "print(len(elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# 2. Create a temporary directory and replicate the structure\n",
    "def create_temp_repo_structure_with_content(paths, base_path):\n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "    for file_path in paths:\n",
    "        # Define the full path in the temporary directory\n",
    "        temp_file_path = Path(temp_dir) / file_path\n",
    "        # Define the original file path\n",
    "        original_file_path = base_path / file_path\n",
    "        # Create necessary directories\n",
    "        temp_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Copy content from the original file if it exists\n",
    "        if original_file_path.exists():\n",
    "            shutil.copyfile(original_file_path, temp_file_path)\n",
    "        else:\n",
    "            # Create an empty file if the original does not exist\n",
    "            temp_file_path.touch()\n",
    "\n",
    "    return temp_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary repo structure created at: C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\n",
      "Contents:\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\fl\\combine.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\fl\\FL.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\fl\\Index.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\fl\\localize.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\fl\\retrieve.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\repair\\repair.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\repair\\rerank.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\test\\generate_reproduction_tests.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\test\\run_regression_tests.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\test\\run_reproduction_tests.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\test\\run_tests.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\test\\select_regression_tests.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\util\\api_requests.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\util\\compress_file.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\util\\index_skeleton.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\util\\model.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\util\\parse_global_var.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\util\\postprocess_data.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\util\\preprocess_data.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\agentless\\util\\utils.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\classification\\graph_classification.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\classification\\load_filtered_benchmark.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\dev\\util\\cost.py\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\tmpbtp9q5lo\\Agentless\\get_repo_structure\\get_repo_structure.py\n"
     ]
    }
   ],
   "source": [
    "# Create the temp directory with file contents\n",
    "base_path = Path(\"D:\\Projects\\llmpairprog\")\n",
    "temp_dir = create_temp_repo_structure_with_content(elements, base_path)\n",
    "\n",
    "print(f\"Temporary repo structure created at: {temp_dir}\")\n",
    "print(\"Contents:\")\n",
    "for root, dirs, files in os.walk(temp_dir):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import ast\n",
    "# from get_repo_structure.get_repo_structure import get_project_structure_from_scratch\n",
    "# PROJECT_FILE_LOC = os.environ.get(\"PROJECT_FILE_LOC\", None)\n",
    "# print(PROJECT_FILE_LOC)\n",
    "\n",
    "def get_docstring(node):\n",
    "    \"\"\"Extract docstring from AST node if it exists.\"\"\"\n",
    "    if (\n",
    "        node.body \n",
    "        and isinstance(node.body[0], ast.Expr) \n",
    "        and isinstance(node.body[0].value, ast.Str)\n",
    "    ):\n",
    "        return node.body[0].value.s\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_function_signature(node):\n",
    "    \"\"\"Extract function signature from AST node.\"\"\"\n",
    "    args_list = []\n",
    "    \n",
    "    # Get positional args\n",
    "    for arg in node.args.posonlyargs:\n",
    "        args_list.append(arg.arg)\n",
    "        \n",
    "    # Get regular args\n",
    "    for arg in node.args.args:\n",
    "        args_list.append(arg.arg)\n",
    "        \n",
    "    # Get args with defaults\n",
    "    defaults = [None] * (len(node.args.args) - len(node.args.defaults)) + node.args.defaults\n",
    "    for arg, default in zip(node.args.args, defaults):\n",
    "        if default:\n",
    "            try:\n",
    "                default_value = ast.literal_eval(default)\n",
    "                args_list.append(f\"{arg.arg}={default_value}\")\n",
    "            except:\n",
    "                args_list.append(f\"{arg.arg}=...\")\n",
    "\n",
    "    # Get *args\n",
    "    if node.args.vararg:\n",
    "        args_list.append(f\"*{node.args.vararg.arg}\")\n",
    "\n",
    "    # Get kwargs\n",
    "    for kwarg in node.args.kwonlyargs:\n",
    "        args_list.append(kwarg.arg)\n",
    "\n",
    "    # Get **kwargs\n",
    "    if node.args.kwarg:\n",
    "        args_list.append(f\"**{node.args.kwarg.arg}\")\n",
    "        \n",
    "    docstring = get_docstring(node)\n",
    "    signature = f\"{node.name}({', '.join(args_list)})\"\n",
    "    \n",
    "    if docstring:\n",
    "        signature += f\"\\n    \\\"\\\"\\\"{docstring}\\\"\\\"\\\"\"\n",
    "    \n",
    "    return signature\n",
    "\n",
    "def parse_python_file(file_path, file_content=None):\n",
    "    \"\"\"Parse a Python file to extract class and function definitions with their line numbers.\n",
    "    :param file_path: Path to the Python file.\n",
    "    :return: Class names, function names, and file contents\n",
    "    \"\"\"\n",
    "    if file_content is None:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                file_content = file.read()\n",
    "                parsed_data = ast.parse(file_content)\n",
    "        except Exception as e:  # Catch all types of exceptions\n",
    "            print(f\"Error in file {file_path}: {e}\")\n",
    "            return [], [], \"\"\n",
    "    else:\n",
    "        try:\n",
    "            parsed_data = ast.parse(file_content)\n",
    "        except Exception as e:  # Catch all types of exceptions\n",
    "            print(f\"Error in file {file_path}: {e}\")\n",
    "            return [], [], \"\"\n",
    "\n",
    "    class_info = []\n",
    "    function_names = []\n",
    "    class_methods = set()\n",
    "\n",
    "    for node in ast.walk(parsed_data):\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            methods = []\n",
    "            for n in node.body:\n",
    "                if isinstance(n, ast.FunctionDef):\n",
    "                    methods.append(\n",
    "                        {\n",
    "                            \"name\": n.name,\n",
    "                            \"signature\": \"@classmethod\\ndef \" + get_function_signature(n),\n",
    "                            \"start_line\": n.lineno,\n",
    "                            \"end_line\": n.end_lineno\n",
    "                            # \"text\": file_content.splitlines()[\n",
    "                            #     n.lineno - 1 : n.end_lineno\n",
    "                            # ],\n",
    "                        }\n",
    "                    )\n",
    "                    class_methods.add(n.name)\n",
    "            class_info.append(\n",
    "                {\n",
    "                    \"name\": node.name,\n",
    "                    \"start_line\": node.lineno,\n",
    "                    \"end_line\": node.end_lineno,\n",
    "                    # \"text\": file_content.splitlines()[\n",
    "                    #     node.lineno - 1 : node.end_lineno\n",
    "                    # ],\n",
    "                    \"methods\": methods\n",
    "                }\n",
    "            )\n",
    "        elif isinstance(node, ast.FunctionDef) and not isinstance(\n",
    "            node, ast.AsyncFunctionDef\n",
    "        ):\n",
    "            if node.name not in class_methods:\n",
    "                function_names.append(\n",
    "                    {\n",
    "                        \"name\": node.name,\n",
    "                        \"signature\": get_function_signature(node),\n",
    "                        \"start_line\": node.lineno,\n",
    "                        \"end_line\": node.end_lineno\n",
    "                        # \"text\": file_content.splitlines()[\n",
    "                        #     node.lineno - 1 : node.end_lineno\n",
    "                        # ],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return class_info, function_names#, file_content.splitlines()\n",
    "\n",
    "def create_structure(directory_path):\n",
    "    \"\"\"Create the structure of the repository directory by parsing Python files.\n",
    "    :param directory_path: Path to the repository directory.\n",
    "    :return: A dictionary representing the structure.\n",
    "    \"\"\"\n",
    "    structure = {}\n",
    "\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        repo_name = os.path.basename(directory_path)\n",
    "        # print(\"repo name\", repo_name)\n",
    "        relative_root = os.path.relpath(root, directory_path)\n",
    "        if relative_root == \".\":\n",
    "            relative_root = repo_name\n",
    "        curr_struct = structure\n",
    "        for part in relative_root.split(os.sep):\n",
    "            if part not in curr_struct:\n",
    "                curr_struct[part] = {}\n",
    "            curr_struct = curr_struct[part]\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                class_info, function_names = parse_python_file(file_path)\n",
    "                curr_struct[file_name] = {\n",
    "                    \"classes\": class_info,\n",
    "                    \"functions\": function_names\n",
    "                    # \"text\": file_lines,\n",
    "                }\n",
    "            else:\n",
    "                curr_struct[file_name] = {}\n",
    "\n",
    "    return structure\n",
    "\n",
    "# structure = create_structure(r\"D:\\Projects\\llmpairprog\\Agentless\\playground\\588436a5-43f1-45f4-80b4-0394f1d7b838\")\n",
    "# print(structure['matplotlib']['setup.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = create_structure(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': [{'end_line': 25,\n",
      "              'methods': [{'end_line': 21,\n",
      "                           'name': '__init__',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def __init__(self, instance_id, '\n",
      "                                        'structure, problem_statement, '\n",
      "                                        '**kwargs)',\n",
      "                           'start_line': 18},\n",
      "                          {'end_line': 25,\n",
      "                           'name': 'localize',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def localize(self, top_n, mock, '\n",
      "                                        'top_n=1, mock=False)',\n",
      "                           'start_line': 24}],\n",
      "              'name': 'FL',\n",
      "              'start_line': 17},\n",
      "             {'end_line': 792,\n",
      "              'methods': [{'end_line': 240,\n",
      "                           'name': '__init__',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def __init__(self, instance_id, '\n",
      "                                        'structure, problem_statement, '\n",
      "                                        'model_name, backend, logger, '\n",
      "                                        '**kwargs)',\n",
      "                           'start_line': 226},\n",
      "                          {'end_line': 244,\n",
      "                           'name': '_parse_model_return_lines',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def _parse_model_return_lines(self, '\n",
      "                                        'content)',\n",
      "                           'start_line': 242},\n",
      "                          {'end_line': 311,\n",
      "                           'name': 'localize_irrelevant',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def localize_irrelevant(self, top_n, '\n",
      "                                        'mock, top_n=1, mock=False)',\n",
      "                           'start_line': 246},\n",
      "                          {'end_line': 361,\n",
      "                           'name': 'localize',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def localize(self, top_n, mock, '\n",
      "                                        'top_n=1, mock=False)',\n",
      "                           'start_line': 313},\n",
      "                          {'end_line': 460,\n",
      "                           'name': 'localize_function_from_compressed_files',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def '\n",
      "                                        'localize_function_from_compressed_files(self, '\n",
      "                                        'file_names, mock, temperature, '\n",
      "                                        'keep_old_order, compress_assign, '\n",
      "                                        'total_lines, prefix_lines, '\n",
      "                                        'suffix_lines, mock=False, '\n",
      "                                        'temperature=0.0, '\n",
      "                                        'keep_old_order=False, '\n",
      "                                        'compress_assign=False, '\n",
      "                                        'total_lines=30, prefix_lines=10, '\n",
      "                                        'suffix_lines=10)',\n",
      "                           'start_line': 363},\n",
      "                          {'end_line': 544,\n",
      "                           'name': 'localize_function_from_raw_text',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def '\n",
      "                                        'localize_function_from_raw_text(self, '\n",
      "                                        'file_names, mock, temperature, '\n",
      "                                        'keep_old_order, mock=False, '\n",
      "                                        'temperature=0.0, '\n",
      "                                        'keep_old_order=False)',\n",
      "                           'start_line': 462},\n",
      "                          {'end_line': 681,\n",
      "                           'name': 'localize_line_from_coarse_function_locs',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def '\n",
      "                                        'localize_line_from_coarse_function_locs(self, '\n",
      "                                        'file_names, coarse_locs, '\n",
      "                                        'context_window, add_space, '\n",
      "                                        'sticky_scroll, no_line_number, '\n",
      "                                        'temperature, num_samples, mock, '\n",
      "                                        'keep_old_order, temperature=0.0, '\n",
      "                                        'num_samples=1, mock=False, '\n",
      "                                        'keep_old_order=False)',\n",
      "                           'start_line': 546},\n",
      "                          {'end_line': 792,\n",
      "                           'name': 'localize_line_from_raw_text',\n",
      "                           'signature': '@classmethod\\n'\n",
      "                                        'def localize_line_from_raw_text(self, '\n",
      "                                        'file_names, mock, temperature, '\n",
      "                                        'num_samples, keep_old_order, '\n",
      "                                        'mock=False, temperature=0.0, '\n",
      "                                        'num_samples=1, keep_old_order=False)',\n",
      "                           'start_line': 683}],\n",
      "              'name': 'LLMFL',\n",
      "              'start_line': 28}],\n",
      " 'functions': [{'end_line': 405,\n",
      "                'name': 'message_too_long',\n",
      "                'signature': 'message_too_long(message)',\n",
      "                'start_line': 402},\n",
      "               {'end_line': 489,\n",
      "                'name': 'message_too_long',\n",
      "                'signature': 'message_too_long(message)',\n",
      "                'start_line': 486},\n",
      "               {'end_line': 587,\n",
      "                'name': 'message_too_long',\n",
      "                'signature': 'message_too_long(message)',\n",
      "                'start_line': 584},\n",
      "               {'end_line': 713,\n",
      "                'name': 'message_too_long',\n",
      "                'signature': 'message_too_long(message)',\n",
      "                'start_line': 710}]}\n"
     ]
    }
   ],
   "source": [
    "pprint(structure['Agentless']['agentless']['fl']['FL.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Agentless\\\\agentless\\\\fl\\\\FL.py',\n",
       " 'Agentless\\\\agentless\\\\fl\\\\Index.py',\n",
       " 'Agentless\\\\agentless\\\\fl\\\\combine.py',\n",
       " 'Agentless\\\\agentless\\\\fl\\\\localize.py',\n",
       " 'Agentless\\\\agentless\\\\fl\\\\retrieve.py',\n",
       " 'Agentless\\\\agentless\\\\repair\\\\repair.py',\n",
       " 'Agentless\\\\agentless\\\\repair\\\\rerank.py',\n",
       " 'Agentless\\\\agentless\\\\test\\\\generate_reproduction_tests.py',\n",
       " 'Agentless\\\\agentless\\\\test\\\\run_regression_tests.py',\n",
       " 'Agentless\\\\agentless\\\\test\\\\run_reproduction_tests.py']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
